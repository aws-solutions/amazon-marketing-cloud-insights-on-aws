# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

#######################################################
# Transformation script where compressed files
# are unzipped and uploaded along with relevent 
# metadata to a staging transformation bucket.
#######################################################

import awswrangler as wr
import re
from aws_solutions.core.helpers import get_service_resource
import gzip
import json
from io import BytesIO, TextIOWrapper

#######################################################
# Use S3 Interface to interact with S3 objects
# For example to download/upload them
#######################################################
from datalake_library.commons import init_logger
from datalake_library.configuration.resource_configs import S3Configuration, KMSConfiguration

s3 = get_service_resource('s3')

logger = init_logger()


class CustomTransform():
    def __init__(self):
        logger.info("S3 Blueprint Light Transform initiated")

    @staticmethod
    def read_gzip(s3_object_data: dict) -> str:
        # binary object data is retieved from s3, decompressed, and converted to json text
        gzip_data = s3_object_data['Body'].read()
        with gzip.GzipFile(fileobj=BytesIO(gzip_data), mode='rb') as f:
            with TextIOWrapper(f, encoding='utf-8') as text_file:
                json_data = json.load(text_file)
        return json.dumps(json_data)

    def transform_object(self, resource_prefix, bucket, key, team, dataset) -> list:
        stage_bucket = S3Configuration(resource_prefix).stage_bucket

        logger.info(f"SOURCE OBJECT KEY: {key}")

        s3_object = s3.Object(bucket, key)

        # our incoming objects are written to s3 in the following format : {team}/{dataset}/{table}/{file_name}.{file_extension}
        key_parts = re.match(r'[^/]+/[^/]+/(?P<table>[^/]+)/(?P<file_name>[^/]+)\.(?P<file_extension>[^/]+)', key)

        logger.info(f"team: {team}")
        logger.info(f"dataset: {dataset}")

        # table name if amazon ads dataset, table prefix if selling partner
        table = key_parts.group('table')
        sanitized_table = wr.catalog.sanitize_table_name(table) # sanitize the table name with values allowed in Glue
        if table == sanitized_table:
            logger.info(f"table: {table}")
        else:
            logger.info(f"original table: {table}, sanitized table: {sanitized_table}")
            table = sanitized_table

        # file name
        file_name = key_parts.group('file_name')
        logger.info(f"file_name: {file_name}")

        # file extension
        file_extension = key_parts.group('file_extension')
        supported_extensions = ['gz', 'json']
        if file_extension.lower() not in supported_extensions: # check that the file extension is supported
            logger.error(f'Unsupported file extension: {file_extension}. Supported types: {supported_extensions}')
            return []
        else:
            logger.info(f"file_extension : {file_extension}")
            
        # get object data from s3
        s3_object_data = s3_object.get()
        
        # we check for timestamp metadata in the object data.
        # we expect it to always be present as long as the file was downloaded by the reporting microservice,
        # but fall back on using the LastModified date generated by s3 if missing
        timestamp = s3_object_data.get('Metadata', {}).get('timestamp')
        if not timestamp:
            logger.info("No timestamp found in object metadata, using LastModified date")
            timestamp = s3_object_data.get('LastModified')
            timestamp = timestamp.isoformat()

        # read the output data from the s3 object after first checking the filetype
        output_data = None
        if file_extension == "gz":
            output_data = self.read_gzip(s3_object_data)
            output_file = file_name # {file_name} already includes json extension

        elif file_extension == "json":
            output_data = s3_object_data['Body'].read().decode('utf-8')
            output_file = f"{file_name}.json" # {file_name} does not include json extension

        # check if report data file is empty
        if not json.loads(output_data):
            logger.info("File empty: No data to process")
            return []

        # construct destination output s3 key
        s3_output_key = f"pre-stage/{team}/{dataset}/{table}/{output_file}"
        logger.info(f"s3_output_key: {s3_output_key}")

        # write object to destination s3 bucket
        kms_key = KMSConfiguration(resource_prefix, "Stage").get_kms_arn
        s3.Object(stage_bucket, s3_output_key).put(
            Body=output_data, 
            ServerSideEncryption='aws:kms', 
            SSEKMSKeyId=kms_key,
            Metadata={
                'timestamp': timestamp
            }
        )
        
        #######################################################
        # IMPORTANT
        # This function must return a Python list
        # of transformed S3 paths. Example:
        # ['pre-stage/engineering/legislators/persons_parsed.json']
        #######################################################

        return [s3_output_key]
